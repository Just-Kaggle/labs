{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('../kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼에서 우리는 베이직한 자연어처리(NLP) 기법 몇 개를 다룰 것입니다. 다룰 목록은 다음과 같습니다:\n",
    "* 유용한 일부 NLP 라이브러리 & 데이터셋 읽기\n",
    "* 각 작가들이 각 단어를 사용한 빈도수 찾기\n",
    "* 어느 작가가 문장을 썼는지 추측하는 데에 단어 빈도수 사용하기\n",
    "\n",
    "준비되었나요? 시작해봅시다! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "해당 튜토리얼에서는 정규화된 유니그램 빈도를 기준으로 어느 작가가 텍스트 문자열을 작성했는지 추측합니다. 이는 각 작가가 훈련 데이터에서 모든 단어를 얼마나 자주 사용하는지를 세고 그들이 쓴 총 단어 갯수로 나누는 멋진 방법입니다. 그런 다음, 테스트 문장에서 한 작가가 다른 작가들보다 더 많은 단어를 사용하는 것을 본다면 우리는 이 작가가 그 작가임을 추측할 수 있습니다.\n",
    "다음을 우리의 트레이닝 말뭉치라고 가정해봅시다:\n",
    "\n",
    "* 작가 1: \"A very spooky thing happened. The thing was so spooky I screamed.\"\n",
    "* 작가 2: \"I ate a tasty candy apple. It was delicious\"\n",
    "\n",
    "그리고 다음은 어느 작가가 썼는지 알아내고 싶은 테스트 문장입니다:\n",
    "\n",
    "* 작가 ??? : \"What a spooky thing!\"\n",
    "\n",
    "그냥 봐서는, 이 문장은 작가 1이 쓴 것 같습니다. 작가 1은 'spooky'와 'thing'을 둘 다 많이 쓰는 반면, 작가 2는 그렇지 않습니다(적어도 우리의 훈련 데이터에 따르면). 테스트 문장에서 'spooky'와 'thing'을 둘 다 볼 수 있기 때문에, 이는 작가 2보다는 작가 1이 쓴 것이라고 볼 수 있습니다 -- 비록 테스트 문장에는 'a'라는 단어가 있고 작가 2도 이를 사용했지만 말입니다. \n",
    "\n",
    "튜토리얼의 나머지 부분에서는 이 직관을 코드로 변환할 수 있는지 알아볼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유용한 NLP 라이브러리 & 데이터셋\n",
    "\n",
    "이 튜토리얼에서는 'NLTK' 라고 하는 Natural Language Toolkit을 사용하겠습니다. 이는 언어 데이터를 분석하기 위한 오픈 소스 파이썬 라이브러리입니다. NLTK의 좋은 점은 많은 일반적인 NLP 작업을 단계별로 진행하는 유용한 책이 있다는 것입니다. 더 좋은 점은 여기에서 책을 무료로 얻을 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk # the natural language toolkit, open-source NLP\n",
    "import pandas as pd # dataframes\n",
    "import zipfile\n",
    "\n",
    "### Read in the data\n",
    "\n",
    "# read our data into a dataframe\n",
    "df = pd.DataFrame()\n",
    "#pd.read_csv() - df = pd.read_csv('../input/spooky-author-identification/train.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "Dataset = 'train'\n",
    "\n",
    "# Will unzip the files so that you can see them..\n",
    "with zipfile.ZipFile(\"../input/spooky-author-identification/\"+Dataset+\".zip\",\"r\") as z:\n",
    "    z.extractall(\".\")\n",
    "\n",
    "texts = pd.read_csv(Dataset + '.csv')\n",
    "\n",
    "# look at the first few rows\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.listdir('../input/spooky-author-identification/train.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 작가가 각 단어를 사용한 빈도 수 알아보기\n",
    "\n",
    "각 작가가 각 단어를 얼마나 자주 사용하는지 알아보십시오. 많은 NLP 응용 프로그램은 특정 단어가 얼마나 자주 사용되는지 계산하는 데 의존합니다. (이 용어에 대한 멋진 용어는 '단어 빈도' 입니다.) 데이터셋에서 각 저자의 단어 빈도를 살펴보겠습니다. NLTK에는 이를 위해 사용할 수 있는 멋진 내장함수와 데이터 구조가 많이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = texts.groupby('author')\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so 'The' and 'the' get counted as\n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens\n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "    \n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distribution\n",
    "# of words for a specific author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 각 작가가 특정 단어를 어느 빈도로 사용하는지 볼 수 있습니다. 이것이 할로윈 대회용이니까 'blood', 'scream', 'fear' 같은 건 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood: EAP\n",
      "0.00014646397201676582\n",
      "blood: HPL\n",
      "0.00022992337803427008\n",
      "blood: MWS\n",
      "0.00022773011333545174\n",
      "\n",
      "scream: EAP\n",
      "1.7231055531384214e-05\n",
      "scream: HPL\n",
      "9.196935121370803e-05\n",
      "scream: MWS\n",
      "2.6480245736680435e-05\n",
      "\n",
      "fear: EAP\n",
      "0.00010338633318830528\n",
      "fear: HPL\n",
      "0.0005748084450856752\n",
      "fear: MWS\n",
      "0.0006196377502383222\n"
     ]
    }
   ],
   "source": [
    "# see how often each author says 'blood'\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print('blood: ' + i)\n",
    "    print(wordFreqByAuthor[i].freq('blood'))\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says 'scream'\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print('scream: ' + i)\n",
    "    print(wordFreqByAuthor[i].freq('scream'))\n",
    "    \n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says 'fear'\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print('fear: ' + i)\n",
    "    print(wordFreqByAuthor[i].freq('fear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어느 작가가 문장을 썼는지 추측하는 데에 단어 빈도수 사용하기\n",
    "\n",
    "일반적인 아이디어는 다른 사람들은 다른 단어들을 더 자주 또는 덜 사용하는 경향이 있다는 것입니다. (나의 경우에는 특히 'gestalt'를 좋아하셨던 교수님이 계셨습니다.) 누가 무엇을 말했는지 확실하지 않지만 그 안에 한 사람이 많이 사용하는 단어가 많을 경우, 그 단어들은 어떤 한 사람이 썼을 것이라고 추측 가능합니다.\n",
    "\n",
    "이 일반적인 원칙을 사용하여 누가 'It was a dark and stormy night.'이라는 문장을 쓸 가능성이 더 높을지 추측해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to guess authorship is to use the joint probability that each\n",
    "# author used each word in a given sentence\n",
    "\n",
    "# first, let's start with a test sentence\n",
    "testSentence = \"It was a dark and stormy night.\"\n",
    "\n",
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empty dataframe to put our output in\n",
    "testProbabilities = pd.DataFrame(columns = ['author', 'word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence..\n",
    "    for j in preProcessedTestSentence:\n",
    "        # find out how frequentyly the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i,j,smoothedWordFreq]], columns = ['author', 'word','probability'])\n",
    "        testProbabilities = testProbabilities.append(output, ignore_index = True)\n",
    "\n",
    "# empty dataframe for the probability that each author wrote the snetence\n",
    "testProbabilitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbabilities.query('author == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "    testProbabilitiesByAuthor = testProbabilitiesByAuthor.append(output, ignore_index=True)\n",
    "    \n",
    "# and our winner is...\n",
    "testProbabilitiesByAuthor.loc[testProbabilitiesByAuthor['jointProbability'].idxmax(),'author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 훈련 데이터에서 본 것을 바탕으로, 세 명의 작가들 중 H.P.Lovecraft가 'It was a dark and stormy night\" 라는 문장을 썼을 가능성이 높은 것으로 나타납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testProbabilitiesByAuthor.to_csv(\"testProbabilityByAuthor.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
