{"cells":[{"metadata":{"_uuid":"0ce8368deb46a77d2115dbc3d37859e8aa20c953","_cell_guid":"c986a9fb-e199-40eb-947d-c398d54f6b1e"},"cell_type":"markdown","source":"# 예측을 위한 EDA (DieTanic)\n"},{"metadata":{},"cell_type":"markdown","source":"EDA(Exploratory data analysis, 탐색적 자료 분석): 수집한 데이터를 다양한 관점에서 관찰하고 이해하는 과정, 그래프나 통계적인 방법으로 자료를 직관적으로 바라보는 과정이다.\n\n과정: 데이터 가져오기 - 데이터 모양 확인 - 데이터 타입 확인 - 데이터 기초 분석 - 데이터 클린징 - 데이터 시각화 - 의사결정 EDA\n\n4가지 주제\n1. 저항성: 자료의 일부가 완전히 다른 값으로 대체되거나 파손되었을 때, 영향받는 정도. 저항성이 높으면, 일부가 훼손되더라도, 유의미한 정보 검출 가능\n2. 잔차의 해석: 잔차는 주경향으로 부터 얼마나 벗어났는 지를 말해준다. 비정상적인 수치가 나왔을 때, 그 원인을 파악하는 것\n3. 자료의 재표현: 자료 분석을 단순화하기 위해, 로그, 제곱, 역수 등을 취해 적당한 척도로 바꾸는 것\n4. 자료의 현시성: 자료의 정보를 시각적으로 표현하여, 구조를 효율적으로 판단하게 해주는 것\n"},{"metadata":{"_uuid":"6255979b94583a7f5a069ef5dd551491c9df01f0","_cell_guid":"33cace66-52e3-4466-929c-6a506e6483ba"},"cell_type":"markdown","source":"### *때때로 삶은 가능한 최악의 순간에 원하는 것을 주는 잔인한 유머 감각이 있다.*\n                                                                                       -리사 클레이파스\n\n                                                                                                                                     "},{"metadata":{"_uuid":"925765e573c2665df48f766467ed75eaab81190c","_cell_guid":"0bef0e9b-81e0-4737-b972-9cb8a06b6b63"},"cell_type":"markdown","source":"타이타닉의 침몰은 역사에서 가장 악명높은 난파선 중 하나이다. 1912년 4월 15일, 첫 출항에서, 타이타닉은 빙산과 충돌 후에 가라앉았고, 2224명의 승객 중 1502명의 승객과 선원이 죽었다. 이것이 타이타닉을 **다이(Die)타닉**인 이유이다. 아무도 잊을 수 없고, 정말 잊지 못 할 재앙이다.\n\n타이타닉을 짓는 데, 750만 달러가 소요되었는데, 충돌 때문에 바다에 가라앉았다. 타이타닉 Dataset은 초보자들이 데이터 사이언스에 첫 시작 및 kaggle에서 competitions에 참여하기 매우 좋은 Dataset이다.\n\n이 notebook의 목적은 **예측가능한 모델링 문제에서 어떻게 작업 흐름을 가져갈 지에 대한 아이디어**를 주는 것이다. 어떻게 우리는 feature를 확인하고, 어떻게 우리는 새로운 feature와 어떤 머신 러닝 개념을 추가할 것인가. 나는 처음 하는 사람도 모든 구문을 이해할 수 있게 하기 위해 가능한 기초적인 notebook을 유지하려고 노력했다.\n\n만약 notebook이 좋고, 도움이 된다면, **upvote를 부탁한다**. 이것은 나에게 동기부여가 된다."},{"metadata":{"_uuid":"8fa2571b91b93e0b0a08b7a9e4eedc060ba76c20","_cell_guid":"706b0b7c-19f4-41c9-865f-5b3375253e0a"},"cell_type":"markdown","source":"## Notebook 내용:\n\n#### Part1: Exploratory Data Analysis(EDA):\n1)Analysis of the features.\n\n2)Finding any relations or trends considering multiple features.\n#### Part2: Feature Engineering and Data Cleaning:\n1)Adding any few features.\n\n2)Removing redundant features.\n\n3)Converting features into suitable form for modeling.\n#### Part3: Predictive Modeling\n1)Running Basic Algorithms.\n\n2)Cross Validation.\n\n3)Ensembling.\n\n4)Important Features Extraction."},{"metadata":{"_uuid":"18ba4a8f0909fd0a758b8cb8717327de8aeacdc8","_cell_guid":"bf5980c3-b168-4a26-81f3-c7bdcedb6429"},"cell_type":"markdown","source":"## Part1: Exploratory Data Analysis(EDA)"},{"metadata":{"_uuid":"7bb401b4e2e509cc8a53e9cf645226fa508fa2e2","collapsed":true,"_cell_guid":"d7601bd6-d22f-499f-97b9-85e01d390f05","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc88acccbc14bf28e4e4d49eee1ea82dcc31ab4","collapsed":true,"_cell_guid":"c12ac199-e3e3-4372-a747-baa6273f4561","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bad00c321363076e535c13a1ad70026c17042f3","_cell_guid":"03e86158-c720-48f7-8dde-ee11f79b7893","trusted":true,"collapsed":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1496190095fb1cd2e289c63c986c6eb951046860","_cell_guid":"5ef569cd-e99e-42f0-93ec-abbc6a90c00e","trusted":true,"collapsed":true},"cell_type":"code","source":"data.isnull().sum() #null 값들의 개수를 확인","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcedf70d9bdab89fb7eb8ff3769b14f0b5036a33","_cell_guid":"31972ab9-edef-49e3-bcd5-120262cf00d8"},"cell_type":"markdown","source":"**Age, Cabin 과 Embarked** 이 null 값들을 갖고 있으므로, 고칠 것이다."},{"metadata":{"_uuid":"d433fbf891d9268f60bf395d7db4e61996989d04","_cell_guid":"841dc40d-06b4-4010-b996-8d1e23857341"},"cell_type":"markdown","source":"### 얼마나 살아남았나??"},{"metadata":{"_uuid":"c60257aef24e867113873729829c7a1e33f4a0ab","_cell_guid":"fabb7625-a8ef-4f37-99c6-3ec93679ef1f","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11fa0236baf6d291f1a1f9325cc42d71fc1b61c1","_cell_guid":"5be22cbd-9b03-4e9f-8eaf-068dd9df401f"},"cell_type":"markdown","source":"많은 승객들이 생존한 사고가 아니라는 증거이다.\n\ntraining set에서 891명의 승객 중, 오직 350명의 승객만 살아남았다. 즉, 전체 training set 중 **38.4%** 이 사고에서 살아남은 것이다. 데이터에서 더 나은 통찰을 얻기 위해서는 더 조사해야하고, 어떤 카테고리의 승객들의 생존했고 못 했는 지를 알 필요가 있다.\n\ndataset의 다른 feature를 사용하여 생존률을 확인할 것이다. 성별, 승선 항구, 나이 등의 특징들.\n\n먼저, 다른 종류의 feature 들을 이해해야 한다."},{"metadata":{"_uuid":"01e521761a33c3bc2961e2613f729c164269ee51","_cell_guid":"f16f40df-3681-4330-ba57-2955094a6546"},"cell_type":"markdown","source":"## Features의 종류\n\n### Categorical Features:\n범주형  변수는 두 개 이상의 범주를 가지고 있고, feature의 각각의 값이 범주로 분류될 수 있다. 예를 들어, 성별은 2개의 범주(남성, 여성)를 가지고 있는 범주형 변수입니다. 그리고 변수들을 분류하거나 순서를 정할 수 없습니다. 이것은 또한 **Nominal Variables(명목 변수)**라고도 합니다.\n\n**Categorical Features in the dataset: Sex,Embarked.**\n\n### Ordinal Features:\n순서형 변수는 범주형 값과 비슷하다. 그러나 둘의 차이점은 값들을 상대적으로 순위를 매기거나 분류를 할 수 있다는 것이다. 예를 들어, 만약 **높이** 같은 feature는 **큼, 중간, 작음**의 값을 가진다면, 높이는 순서형 변수이다. 여기에서 변수는 상대적인 분류을 가질 수 있다.\n\n**Ordinal Features in the dataset: PClass**\n\n### Continous Feature:\nfeature가 feature 열에서 최소,최대값같은 2개의 점에서 값을 가진다면, 연속적이라고 한다.\n\n**Continous Features in the dataset: Age**"},{"metadata":{"_uuid":"2b36f7862279cf64a76a9950f703bfed4ca220f6","_cell_guid":"ccd13018-e5fb-4022-ac41-cadce1994dbe"},"cell_type":"markdown","source":"## Features 분석하기"},{"metadata":{"_uuid":"8b5ad1ae98e4aad980f24bbefb489e6ac049768b","_cell_guid":"8d5bd219-61ce-4c88-b0c5-aaffce8cb1cc"},"cell_type":"markdown","source":"## 성별--> Categorical Feature"},{"metadata":{"_uuid":"3554e468c8581316a717348689f1d867b3c97f6a","_cell_guid":"428c84fc-9d5e-4022-a9f5-1c8ec7257268","trusted":true,"collapsed":true},"cell_type":"code","source":"data.groupby(['Sex','Survived'])['Survived'].count() #성별별 생존여부","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06e043424e13b87fcb020322e4869430fd0714f","_cell_guid":"06218a7d-bf3c-40b1-9cfa-2a915f7bc005","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebf3e75bbc120054b947162e088876bbaca3cf54","_cell_guid":"d97e7fe8-a98c-40f3-98c7-b5118c0295db"},"cell_type":"markdown","source":"이것은 흥미롭다. 배에서 남성의 수는 여성의 수보다 훨씬 더 많다. 그럼에도 구해진 여성의 수는 구해진 남성의 수의 거의 2배이다. 배에서의 **여성의 생존률을 약 75%인 반면에, 남성의 경우는 약 18~19%** 이다. \n\n이것은 모델링에 **매우 중요한** 특징이다. 그러나 이것이 최선일까? 다른 feature들을 확인하자."},{"metadata":{"_uuid":"e3b6327723dedd766d452f55b4056bbd0b37bed2","_cell_guid":"a210b0c8-dd8e-4fd0-a7f2-e597b7ed81e6"},"cell_type":"markdown","source":"## Pclass --> Ordinal Feature"},{"metadata":{"_uuid":"4a98fe27c4474296c6b51f4a2b7fb076c228b4b8","_cell_guid":"2477b536-32dd-43a0-8824-be034104b760","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"592a3d8c24761c3f6e8c5cf875554826d9e308a5","_cell_guid":"c3adaaa2-f675-4273-ba93-8b26c37bacf3","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace6b99b7d75b76ecead6f03372c1790fe3aa6c7","collapsed":true,"_cell_guid":"eec00d94-07d2-4e6e-8797-cb6f98341793"},"cell_type":"markdown","source":"사람들은 **돈이 모든 것을 살 수는 없다.** 라고 말한다. 그러나 우리는 분명히 Pclass 1의 승객이 구조에서 매우 높은 우선 순위를 가진다는 것을 알 수 있다. 비록 Pclass 3인 승객의 수가 훨씬 더 높음에도 불구하고, 그들의 생존률은 **25%**로 매우 낮다.\n\nPclass1 생존률을 약 **63%**이고 Pclass2는 약 **48%**이다. 그래서 돈과 지위가 중요하다. 그런 물질만능적인 세계이다.\n\n다른 흥미로운 관측을 위해 조금 더 깊이 들어가자. **성별과 Pclass** 의 생존율을 함께 확인하자."},{"metadata":{"_uuid":"1308ec5a68849984dfd1e05b53c52b6192363a18","_cell_guid":"7d413d16-2861-4aca-9042-38e374eddef3","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710111beaace27f0e85958a0639f2b2175b0892c","_cell_guid":"1fd41001-f153-4a78-806b-72b16a34f88f","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dbe07636bbd6d44fb0854c9c9cec03529117042","_cell_guid":"7ae5251a-8bd9-4638-85c8-b0e0ed91420e"},"cell_type":"markdown","source":"우리는 이 경우에 **FactorPlot**을 사용한다, 왜냐하면 범주값 분리를 쉽게 만들 수 있기 때문입니다.\n\n**CrossTab**과 **FactorPlot**을 보면, **Pclass1의 여성**의 생존이 약 **95~96%**가 생존했다는 것을 쉽게 알 수 있고, Pclass1의 94명의 여성 중 오직 3명만 죽었다는 것을 알고 있다.\n\n구조 중에 Pclass와 상관없다는 것과 여성이 최우선이라는 증거입니다. 심지어 Pclass1에서의 남성이 매우 낮은 생존률입니다.\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nPclass와 같은 모습은 또한 중요한 feature 입니다. 다른 feature를 분석해봅시다.\nLooks like Pclass is also an important feature. Lets analyse other features."},{"metadata":{"_uuid":"da1710d88cdb726d4c74d1580eb4650823f8e1a9","_cell_guid":"b9a8739f-9bfa-48a0-8b55-85694f8c7b36"},"cell_type":"markdown","source":"## Age--> Continous Feature\n"},{"metadata":{"_uuid":"58e1110e104a4628f2852fa284905525997c2c44","_cell_guid":"d8c1dc5a-2f74-4c88-9101-6c98abaf9878","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a43082dc51717bbcc0a036db2d78b7732eb41ef1","_cell_guid":"cf3e9729-799e-4142-84ff-0dbb6906136e","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c3ffbed55b33a51666b95a46c9f8aaa634bb73","_cell_guid":"b6f98f68-46c3-4aac-a4c7-ef350bf8ccd7"},"cell_type":"markdown","source":"#### 관측:\n\n1)아이들의 수는 Pclass를 따라 증가하고, 10살 미만의 승객들의 생존율은 Pclass에 상관없이 좋다.\n\n2)Pclass1의 20~50살의 승객의 생존 가능성은 높고, 여성은 훨씬 더 좋다.\n\n3)남성의 경우, 생존 가능성은 나이가 증가할 수록 감소한다."},{"metadata":{"_uuid":"d347a55f54b1ee15ce39ad0e22d873a2c32fc736","_cell_guid":"23be39a5-be98-422b-8116-90cb0fd120ba"},"cell_type":"markdown","source":"앞서 본 것처럼, 나이 feature는 177개의 null 값이 있다. 이 NaN 값들을 대체하기 위해, 그것들을 dataset의 나이의 평균으로 할당할 수 있다.\n\n그러나 문제는, 많은 다른 나이를 가진 많은 사람들이 있다는 것이다. 단순히 4살의 아이를 29살의 평균나이로 할당할 수는 없다. 무슨 연령층의 승객이 거짓말하는 지 알아낼 방법이 있을까?\n\n**빙고!!!!**, **이름** feature를 확인할 수 있다. feature를 보면, 군이나 양 같은 표현을 가지고 있다는 것을 알 수 있다. 따라서, 상대적인 그룹에 군과 양의 평균 값을 할당할 수 있다.\n\n**''What's In A Name??''**---> **Feature**  :p"},{"metadata":{"_uuid":"e92f7a0ef7ea07abb81e602fc700b382c64fca96","collapsed":true,"_cell_guid":"0af8b99e-0d4b-4844-a146-da2a359ca2ff","trusted":true},"cell_type":"code","source":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #인사표현을 추출한다.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413c7cdc7469c86bd9d13fd54c731ecb58704197","_cell_guid":"8efd06d8-dee6-4892-a986-25944cf2bf61"},"cell_type":"markdown","source":"좋다. 여기서는 정규표현식을 사용할 것이다. **[A-Za-z]+)\\.** 그래서 이것이 무엇인가. **A-Z 나 a-z** 사이의 문자열 다음에 **.(점)**이 있는 것을 찾는다. 그래서 성공적으로 이름에서 첫글자들을 추출한다."},{"metadata":{"_uuid":"cf32ee39ef64840facd833f7f5d7616ffbaaa97c","_cell_guid":"e87e0415-43e7-4717-a6dc-43e60e71460e","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #성별과 같은 처음 이름을 확인하기","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0514c0b00c34f72bd77eb597a4e08a1c9edf982","_cell_guid":"1d7d94e4-240e-4cfc-ba21-d036b3bd7869"},"cell_type":"markdown","source":"좋다. Miss 를 나타내는 곳에 Mlle, Mme 와 같은 오타가 있다. 이런 다른 값들을 Miss와 같은 값으로 대체할 것이다."},{"metadata":{"_uuid":"99a86205c88ad2c8fd96fc18225cd10ed91620dd","collapsed":true,"_cell_guid":"55b6028e-948c-4a98-a214-e86212481af4","trusted":true},"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09f5aefc80a89f0f3af69cd9fdf30afb576f456","_cell_guid":"c1d0c1dd-ac10-4360-99e1-dfea7418ad0e","trusted":true,"collapsed":true},"cell_type":"code","source":"data.groupby('Initial')['Age'].mean() #Initials에 따른 평균 나이를 확인하자","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2bbefba6442fcd47e04c90daa43f58fc001e47b","_cell_guid":"57ec5300-f0e3-46ce-a920-6c1846901b6d"},"cell_type":"markdown","source":"### Filling NaN Ages"},{"metadata":{"_uuid":"8bd3c34f7f539bc3d4720531da6405e2d0e96b46","collapsed":true,"_cell_guid":"f006b4b0-a8aa-432c-9bdb-040a435e77f8","trusted":true},"cell_type":"code","source":"## Cell에서 NaN인 값들을 평균나이로 할당하기\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd9e749a4eefeb0b57c0fe97de9b0ee9815c279","_cell_guid":"534ab487-2e49-4df1-93b4-4aac64c52bc1","trusted":true,"collapsed":true},"cell_type":"code","source":"data.Age.isnull().any() #최종적으로 null이 없는 값들","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff9fcc871c21b4949d0082f3609151bb6f3e726","_cell_guid":"b2ed1983-50d5-405c-8bad-c61a087758f5","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e78651053e3b3641da3ded5bac87be689c7df259","_cell_guid":"b4c65724-4641-4e8c-be59-c9d41bfd088b"},"cell_type":"markdown","source":"### Observations:\n1)아기(나이<5)들은 많은 수가 생존했다.(여성과 아이 우선 정책).\n\n2)가장 늙은 승객이 생존했다(80세).\n\n3)최대 사망 수는 30-40 나이 그룹에 있었다."},{"metadata":{"_uuid":"e81923d749fef3cfc374b9b2dcbc9f27e8cc1ecc","_cell_guid":"82ec9949-9681-42d0-959d-02fe4ff2675c","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',col='Initial',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0486c4864f225d54bbc392fa01ec068e9d89d5","_cell_guid":"2e547f77-4b30-4e76-baf8-1ece2f74074d"},"cell_type":"markdown","source":"여성과 아이 우선 정책은 등급과 상관없이 사실이다."},{"metadata":{"_uuid":"a24b323daf19e8fd6cacc83f549523b49a0789e7","_cell_guid":"8be8b82f-8d91-471e-adc0-80dc6d3def0b"},"cell_type":"markdown","source":"## Embarked--> Categorical Value"},{"metadata":{"_uuid":"0d9307f18fcc510c7615e73080f23ba6cb80c3ae","_cell_guid":"ec30e8cc-471d-4616-be83-b01bd45d51a7","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a99134fe7128642103b9b859172b6db85da3514","_cell_guid":"1966db49-4dd7-4b34-98c1-5387f9c3fb70"},"cell_type":"markdown","source":"### Chances for Survival by Port Of Embarkation"},{"metadata":{"_uuid":"8dce8ddd858624321e8a69f2e9a5a30ade19aa12","_cell_guid":"4193b498-a67c-49f1-b6ec-71c4bf0300a1","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Embarked','Survived',data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc3ba850739bf7c24af92b37976d17ac05a668f4","_cell_guid":"d678b40e-ea1a-4340-9c98-562a8550860d"},"cell_type":"markdown","source":"항구 C에서 생존 가능성이 약 0.55로 가장 높고, 반면에 S가 가장 낮다."},{"metadata":{"_uuid":"dfcab3effc1ebf8653e3a4b61149b44fc146fdfb","_cell_guid":"51ff68c3-ffa2-4ac7-95ee-04ecb7d9da64","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57bba2f20422b2db70f0f84e8feb8ba0ace410a0","collapsed":true,"_cell_guid":"6a5b59d7-4886-4b28-9a6d-8cb266f6f0eb"},"cell_type":"markdown","source":"### Observations:\n1)S에서 가장 많은 승객이 탔다. 그들 중 가장 많은 수는 Pclass3이다.\n\n2)C에서 승객들은 생존률이 좋은 것은 운이 좋아보인다. 아마도 이 이유는 Pclass1과 Pclass2 승객들이 모두 구조됐기 때문이다.\n\n3)S 선착장은 대부분 부유한 사람들이 승선한 것 같다. 그럼에도 여기에서 생존율은 낮다. 왜냐하면 Pclass3의 많은 승객들인 약 **81%** 가 생존하지 못 했다.\n\n4)Q 항구는 Pclass3에서 승객들 중 약 95%가 있었다."},{"metadata":{"_uuid":"2546b1329d2f46bbfdc4b6ac3728747c436ee3f1","_cell_guid":"566e32f9-eaa9-44b1-b904-b71f242f7c6e","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a82c6f302df8fb1227ed024f38f2e8a73376d113","collapsed":true,"_cell_guid":"e694b26b-ccca-4405-9fa5-af4f91527d80"},"cell_type":"markdown","source":"### Observations:\n\n1)Pclass에 상관없이 Pclass1과 Pclass2의 여성들의 생존율은 거의 1이다. (Pclass가 아니라 Embark 같은)\n\n2)S 항구는 Pclass3의 남성과 여성 모두의 생존률이 매우 낮은 것으로 보아, 매우 운이 없어보인다. **(돈이 중요하다.)**\n\n3)Q 항구는 남성들에게 가장 운이 없는 것처럼 보인다. 거의 모든 Pclass3인 사람들처럼.\n"},{"metadata":{"_uuid":"c124234128b669e41546daab16bfb85d14b5dc03","collapsed":true,"_cell_guid":"9d2984b4-c9a7-44bf-ada3-78afc83bcd26"},"cell_type":"markdown","source":"### Filling Embarked NaN\n\nS 항구에서 승객들이 가장 많이 승선했기 때문에, NaN을 S로 대체한다."},{"metadata":{"_uuid":"c77ed7f842ec862326ca6b9986e21a0a7d69acff","collapsed":true,"_cell_guid":"62309104-404b-4f79-a50b-1f1747fde9f5","trusted":true},"cell_type":"code","source":"data['Embarked'].fillna('S',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16f31e0c60dc64d638d9505b2eaa855fb20205cc","_cell_guid":"56d6a590-9ab2-4be6-8a90-f0bb9e908cae","trusted":true,"collapsed":true},"cell_type":"code","source":"data.Embarked.isnull().any()# 최종적으로 NaN 값들은 없다.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838c230d6e4027c9f87b702ffd5805284c74ca4d","collapsed":true,"_cell_guid":"05194e42-445e-41a5-9124-e4ec29d1ac2a"},"cell_type":"markdown","source":"## SibSip-->Discrete Feature\n이 feature는 사람이 혼자인지 가족들과 있는 지를 나타낸다.\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife "},{"metadata":{"_uuid":"56069ce478b75673fab78145fb6a6741cad28d76","_cell_guid":"ae7b6019-3162-400f-9746-8d8239049751","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"565dcbd1acdb973ccfacb41d1d509ee3c59cd126","_cell_guid":"e464b8ab-e642-4666-a701-059c1bd3b77b","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=data,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=data,ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcdc00224417620a8805e0e5d0b6e83c81119981","_cell_guid":"c0ce45f8-0b08-4631-ade7-b3ddd3978414","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"485d132cdff1171a3c5853572f6150ad1e4f92cc","collapsed":true,"_cell_guid":"8fd6eeb9-7aed-4bbb-b13d-2d491bdbdd6a"},"cell_type":"markdown","source":"### Observations:\n\nbarplot과 factorplot은 승객이 형제없이 혼자 승성했으면, 34.5%의 생존율이라는 것을 보여준다. 그래프는 만약 형제의 수가 증가하면 매우 감소한다. 이것은 이해가 된다. 다시 말해서, 만약 한 가족이 승선한다면, 자기 자신대신에 가족을 구하려고 할 것이다. 놀랍게도 5-8명의 가족의 생존율은 **0%** 다. 이유는 어쩌면 Pclass 일 지도 모른다??\n\n이유는 **Pclass**다. crosstab은 SibSp>3 인 사람이 모두 Pclass3 이라는 것을 보여준다. Pclass3(>3)인 대가족은 모두 죽음에 임박했다는 것이다."},{"metadata":{"_uuid":"5f4af4fa5a1708815b12e4e0c330a6762647f44b","collapsed":true,"_cell_guid":"4abd6f2c-0b9e-48a8-ba09-97ef75b3499a"},"cell_type":"markdown","source":"## Parch"},{"metadata":{"_uuid":"f426753939cc958e1c358e3cf165c5915a0fcc2d","_cell_guid":"84e405d3-cd3d-4a00-840d-f3e51bbfd45f","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecc5aebc2c638b3a6bbde9338b588d3d803c1af2","_cell_guid":"d07b3740-3af3-442a-b640-54926597c999"},"cell_type":"markdown","source":"crosstab은 Pclass3에 더 큰 가족들이 있다는 것을 다시 보여준다."},{"metadata":{"_uuid":"13b42065a19f14e2ce10dcf597377ea59c9bfc2d","_cell_guid":"fb77d798-a7dc-4483-8ce9-9cf8349934f1","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b55cfb2450f70f31a52c4ed64b89118b7b74ccec","_cell_guid":"4b40e791-0817-4e70-a8e6-c74e121cdf45"},"cell_type":"markdown","source":"### Observations:\n\n여기에서 역시 결과들은 꽤 비슷하다. 부모와 함께 탑승한 승객들은 더 큰 생존 기회를 가지고 있다. 그러나 수가 증가하면, 감소한다.\n\n생존 가능성은 배에 1-3명의 부모가 있는 사람은 좋다. 혼자 있는 것은 치명적이다. 그리고 생존 가능성은 배에 >4인 부모들이 있으면 감소한다."},{"metadata":{"_uuid":"d873672610a96daa00522c850bd1b96013f92856","_cell_guid":"ce242dd4-c537-40f9-8223-76740512e966"},"cell_type":"markdown","source":"## Fare--> Continous Feature"},{"metadata":{"_uuid":"fbd3e42723ae1447bc2f1b91204ffa609b4d07c5","_cell_guid":"db19b152-af6d-41f7-a545-fe41e4be18a8","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Highest Fare was:',data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cce63092c5f0f6864e45e43197a9b9af78a6bd7","_cell_guid":"5ab99107-6bd5-47be-b512-120eab64e7ef"},"cell_type":"markdown","source":"가장 싼 비용은 **0.0** 이다. 무료 호화 탑승이다."},{"metadata":{"_uuid":"cea989cb8581e2d563009339e8194b81c531afde","scrolled":true,"_cell_guid":"e9c4559e-913c-4a2d-9e96-6385e7ed6a06","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f85d68e7c648ffde18f55b6f49701e159256478","_cell_guid":"757f65b4-5a89-4385-9422-a98b2c3999cd"},"cell_type":"markdown","source":"Pclass1에 있는 승객들의 요금이 큰 분포가 있는 것처럼 보이고, 이 분포는 수준이 낮아질 수록 감소한다. 이것은 연속적으로 보인다. 양자화를 사용하여 이산 값으로 바꿀 수 있다."},{"metadata":{"_uuid":"3918cdff2761844f861290010cac76797c2499eb","collapsed":true,"_cell_guid":"de570fa8-3b6c-48b9-908c-6bdb9978bda1"},"cell_type":"markdown","source":"****## 모든 feature를 위해 껍질 속 관측:\n**Sex:** 여성의 생존 가능성이 남성과 비교해서 높다.\n\n**Pclass:** **1등석**이 더 나은 생존율을 갖는다는 뚜렷한 경향이 있다. gives you better chances of survival. **Pclass3**의 생존률이 **매우 낮다.**. **여성**에게는, **Pclass1**의 생존률이 거의 1이고, **Pclass2** 에서의 생존률 또한 매우 높다. **돈이 승리한다!!!**. \n\n**나이:** 5-18세보다 적은 아이들은 정말로 높은 생존율을 갖는다. 15-35세 사이의 승객들은 많이 죽었다.\n\n**Embarked:** 매우 흥미로운 feature다. **S에서 탄 승객들이 대부분 Pclasss1임에도, C에서의 생존율이 더 낫다.** Q에서의 승객들은 모두 **Pclass3**이었다.\n\n**Parch+SibSp:** 1-2 명의 형제나 1-3명의 부모와 같이 탑승한 것은 혼자나 대가족이 여행하는 것 보다 훨씬 더 높은 생존율을 보여준다."},{"metadata":{"_uuid":"3b7ef048f72c226d996c6ad955b2b171d1780b93","_cell_guid":"410e5ca6-2aa4-42a9-9875-d5aeb87d831f"},"cell_type":"markdown","source":"## Correlation Between The Features"},{"metadata":{"_uuid":"afa990766959d5cafd155c0c10c8c2d5afab2919","scrolled":true,"_cell_guid":"88547a8f-28bb-469e-b3b8-5e5fbc4a1e30","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b4721abbc77dcd5dd75616fd0b6f2e9ba198ff4","collapsed":true,"_cell_guid":"e42e7713-4b01-4429-94d9-3dd514227933"},"cell_type":"markdown","source":"### Heatmap 해석\n\n첫번 째로 기억해야하는 것은 알파벳이나 문자 사이를 연관지을 수 없다는 것이 분명한 것 처럼 오직 numeric feature만 가능하다는 것이다. 그래프를 이해하기 전에, 정확히 무엇이 연관있는 지를 봐야한다.\n\n**POSITIVE CORRELATION:**  만약 **A feature의 상승이 B feature의 상승을 이끌어낸다면, 이 둘은 긍정적으로 관계되어 있다고 한다.** 값이 **1이면 완벽한 positive correlation**이다.\n\n**NEGATIVE CORRELATION:** 만약 **A feature의 상승이 B feature의 감소를 이끌어낸다면, 이 둘은 부정적으로 관계되어 있다고 한다.** 값이 **-1이면 완벽한 negative correlation**이다.\n\n지금 두개의 feature들이 몹시 또는 완벽히 연관되어 있어서, 하나의 상승이 다른 것을 상을을 이끌어 내고 있다. 이것은 feature들이 몹시 비슷한 정보를 포함하고 있는 것과 정보에 변화가 매우 작거나 없다는 것 모두를 의미한다. 이것은 양쪽이 거의 같은 정보를 포함하고 있다는 것으로 **MultiColinearity**으로 알려져 있다.\n\n우리는 **그것들 중 하나**는 불필요한 것처럼 사용해야 한다. 모델을 만들거나 학습하는 동안, 학습 시간을 줄이고 많은 그런 이득을 얻기 위해, 불필요한 feature를 제거하려고 노력해야 한다.\n\n위 heatmap처럼, feature들이 많이 연관되어 있지는 않다는 것을 알 수 있다. 가장 높은 correlation은 **SibSp 와 Parch로 0.41** 이다. 그래서 모든 feature를 사용할 수 있다."},{"metadata":{"_uuid":"08132891de3a44c38d099573a8693270b565bb1b","_cell_guid":"734d8511-e8ed-423a-913a-ccaa1ebda241"},"cell_type":"markdown","source":"## Part2: Feature Engineering and Data Cleaning\n\nFeature Engineering이 무엇일까?\n\nfeature에 대한 하나의 데이터셋이 주어질 때마다, 모든 feature가 중요할 것이라는 것은 필요하지 않다. 아마도 제거돼야하는 많은 불필요한 feature들이 있을 지도 모른다. 또한 다른 feature들로 부터 정보를 관측하고 추출함으로써 새로운 feature들을 얻거나 추가할 수 있다.\n\n한 예는 이름 feature를 사용해서 Initals feature를 얻는 것이다. 어떤 새로운 feature를 얻고 몇몇을 제거해야 하는 지를 확인하자. 또한 예측 모델링을 위한 적합한 형태로 존재하는 적정한 feature로 변환하자."},{"metadata":{"_uuid":"488a35e1cce6d5d3a50327ec2dfd9d0961f1abaf","_cell_guid":"f2fe673f-b1e4-4c2b-828c-091f27bbc3f5"},"cell_type":"markdown","source":"## Age_band\n\n#### 나이 feature의 문제:\n**나이는 연속적인 feature**라고 언급했는데, 머신 러닝 모델에서 연속적인 변수는 문제가 있다.\n\n**예:**만약 **성별**로 운동 선수를 그룹짓거나 정렬한다면, 남성과 여성으로 쉽게 구분할 수 있다.\n\n만약 그들의 **나이**로 그룹화한다면, 어떻게 할 것인가? 만약 30명의 사람들이 있다면, 30살의 값이 있을 지도 모른다. 이것은 문제가 있다.\n\nBinning 이나 Normalisation를 사용해서 **continous values를 categorical values**로 변환할 필요가 있다. binning의 예로 한 나이의 범위를 하나의 단일 구간으로 그룹화하거나 하나의 값으로 할당할 수 있다. \n\n승객의 최대 나이는 80이었다. 그래서 5개의 구간으로 범위를 나눈다. 80/5=16이므로,\n구간의 크기는 16이다."},{"metadata":{"_uuid":"f79d6021bde433c229d180a82038bfc061b05093","_cell_guid":"4b641963-d34e-4a4e-972f-df400c21c62d","trusted":true,"collapsed":true},"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f03e02b9ffd6a00041f18f2e34a36f76637101fe","_cell_guid":"2f187870-e106-4852-acf0-9fa2e4115789","trusted":true,"collapsed":true},"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b314354514550835885788e4653c4747ed19bd35","_cell_guid":"ba93248d-da3c-4843-947e-b8fab18e8ab9","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Age_band','Survived',data=data,col='Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10a137ebe2c56afc515342642c35d4daa9b78497","_cell_guid":"b432f71d-6fd1-42be-8533-1b7114276004"},"cell_type":"markdown","source":"생존율은 Pclass와 상관없이 나이가 증가함에 따라 감소한다.\n\n## Family_Size and Alone\n이 쯤에서, 가족수와 개인여부 라고 부를 수 있는 새로운 feature를 만들고 분석할 수 있다. 이 feature는 parch와 sibsp 의 합니다. 우리가 생존률이 승객들의 가족 수와 연관이 있는 지 확인할 수 있는 조합된 데이터를 얻게 된다. 개인은 승객이 혼자인지 아닌지를 나타낸다."},{"metadata":{"_uuid":"89d9a7057f35aaf778a38057bcb6de2cbf6cd1dd","_cell_guid":"c676363a-a754-4fcb-aff3-62422bbc4924","trusted":true,"collapsed":true},"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('Family_Size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.factorplot('Alone','Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81bcb9835ff850fc400183c0c6b599fd4aecd222","_cell_guid":"1ba180b0-c78a-4f9e-a179-6f89464d1c7d"},"cell_type":"markdown","source":"**가족수=0은 승객이 혼자인 것을 의미한다.** 분명히, 만약 혼자이거나 가족수=0 이면, 생존률이 매우 낮다. 가족수>4 이면, 생존율은 역시 감소한다. 이것은 또한 모델에 중요한 feature가 되는 것처럼 보인다. 이것을 더 조사하자."},{"metadata":{"_uuid":"17d8e84e2f6acb5e3b4ae72712002ff6ec267037","_cell_guid":"be014085-aa05-4bd4-8835-76413ada128c","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12d1a4fa19df5691a4ecca4662031a125e98ed02","_cell_guid":"6cf47ab0-5172-499d-ae63-1e45f72175f2"},"cell_type":"markdown","source":"가족과 있는 것보다 혼자인 여성의 생존율이 높은 Pclass3를 제외하면, 혼자있는 것은 성별과 Pclass와 상관없이 해롭다.\n\n## Fare_Range\n\n요금도 연속적인 feature이므로, ordinal value로 변환할 필요가 있다. 이것을 위해 **pands.qcut**을 사용할 것이다.\n\n**qcut**는 우리가 넘겨준 수에 따라서 값을 나누고 정렬한다. 그래서 우리가 5개의 구간을 넘겨준다면, 구간이나 값의 범위를 5개로 나누어서 동등하게 값을 정렬할 것이다."},{"metadata":{"_uuid":"4aec257e46ab71667aed9571663e74f7e1f27d9b","_cell_guid":"f46d820c-e81e-448f-837d-d3dfd63b94e6","trusted":true,"collapsed":true},"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07f991646cc077f42cab42c41942138109af346d","_cell_guid":"357e3fd7-465f-4bd4-83f1-ac78bcdf7538"},"cell_type":"markdown","source":"위에서 살펴본 것처럼, **요금 범위가 증가함에 따라 생존율이 증가한다**라는 것을 분명히 알 수 있다.\n\n요금 범위를 그대로 넘길 수는 없다. 이것을 **age_band**에서 했던 것 처럼 singleton values 로 나눠야 한다."},{"metadata":{"_uuid":"f22ee32844ba93a6162d14ca801385064a996101","collapsed":true,"_cell_guid":"ba80366f-8f8c-421e-bfb9-f3121402e5b6","trusted":true},"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4390d6f0ea444a0a86ce72a592fdb3bd858ac725","_cell_guid":"c3446d11-2151-4cfa-aa43-2637ac793525","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e4dc43d07d286ef9e4e8995d77cddb892973c86","_cell_guid":"2cd0333d-a2dd-4deb-b103-8e2e0fc5da4e"},"cell_type":"markdown","source":"분명히, Fare_cat이 증가함에 따라, 생존률이 증가한다. 이 feature는 성별을 따라 모델링것과 마찬가지로 중요한 feature가 될 지도 모른다.\n\n## Converting String Values into Numeric\n\nstring을 machine learning model에 넘겨줄 수 없기 때문에, 성별, Embarked 등을 numeric values로 바꿀 필요가 있다."},{"metadata":{"_uuid":"e76a3ec186753e9217584419945e6654072819a7","collapsed":true,"_cell_guid":"c9a75f9d-9a0c-4ffc-88ec-5996ee589121","trusted":true},"cell_type":"code","source":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30b66ebbf65acebdefe20a520d9a62762b959590","_cell_guid":"89a1862f-f0ca-48e9-aeaf-460950538c7f"},"cell_type":"markdown","source":"### Dropping UnNeeded Features\n\n**Name**--> 우리는 name feature를 어떤 categorical value로 바꿀 수 없기 때문에 필요하지 않다.\n\n**Age**--> 우리는 age_band feature가 있기 때문에, age가 필요하지 않다.\n\n**Ticket**--> 카테고리화 할 수 없는 random string이다.\n\n**Fare**--> Fare_cat feature가 있기 때문에 필요없다.\n\n**Cabin**--> 많은 NaN value 들과 많은 승객들이 다양한 cabin을 가지고 있다. 그래서  이것은 쓸모없다.\n\n**Fare_Range**--> 우리는 fate_cat feature를 가지고 있다.\n\n**PassengerId**--> 카테고리화 될 수 없다."},{"metadata":{"_uuid":"210bdb7650161cbe09a381dc89c0fc4485b2646c","_cell_guid":"bc04b292-b453-4b45-868b-ab3f99b776dc","trusted":true,"collapsed":true},"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3247092c61752d3330e7a4e4f5e039402f11285","_cell_guid":"ff2e04fa-1213-4183-8f03-d78889bb3aba"},"cell_type":"markdown","source":"위에 있는 correlation plot에서, 양의 상관 관계인 feature들을 볼 수 있다. **SibSp 와 Family_Size** 그리고 **Parch와 Family_Size** 또, **Alone과 Family_Size**처럼 음의 상관 관계도 있다."},{"metadata":{"_uuid":"fb6f48f1da808adbb03cd22b9a823ed2bd3374ae","_cell_guid":"7110f2af-1002-4d78-942b-19318b0d90c3"},"cell_type":"markdown","source":"# Part3: Predictive Modeling\n\nEDA 부분에서 몇몇의 통찰력을 얻었다. 그러나 승객이 생존하고 죽는 지에 대해 정확히 예측하거나 얘기할 수는 없다. 그래서 승객이 생존할 것인지 아닌 지를 몇몇의 훌륭한 Classification Algorithm을 사용해서 예측할 것이다. 다음은 모델을 만들기 위해 사용할 algorithm이다.\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression"},{"metadata":{"_uuid":"c39d18a3cbffb9b7d71b746f5f873ef8e4f7ecb3","collapsed":true,"_cell_guid":"e099ab49-b0b3-40a1-b372-1f006774d641","trusted":true},"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03896d10e6364121c9ffe2a4cfe280353637bcfe","collapsed":true,"_cell_guid":"c01597f4-4536-41d6-8f9a-dad8f955524e","trusted":true},"cell_type":"code","source":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"737c474fe05198067e38559bf0c554af594da5bc","_cell_guid":"6618cea9-bbdc-45d3-b506-12c6000ca7e3"},"cell_type":"markdown","source":"### Radial Support Vector Machines(rbf-SVM)"},{"metadata":{"_uuid":"36939717ad06a4590276530754414bd5a281dde2","_cell_guid":"278c175b-553b-4b79-9e1a-9f4f86b385fd","trusted":true,"collapsed":true},"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2a394ac6de1d870ff4a68f46c7a14356ed65770","_cell_guid":"095e2680-800c-42fe-bff3-6ff7431e41a4"},"cell_type":"markdown","source":"### Linear Support Vector Machine(linear-SVM)"},{"metadata":{"_uuid":"0d8b19eaf5d0ca739d80d264b5a8dec2747a808b","_cell_guid":"a61bf24a-1952-474f-8596-31bf61140e17","trusted":true,"collapsed":true},"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e40b35d7a0c8cdbfab6da2c0f40f062e63e4e6","_cell_guid":"05d907c6-7751-458b-b475-e95f689b9590"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"_uuid":"c3f3229fede1a5869f5c36503acf9dad53dcbcfa","_cell_guid":"80a062ce-b946-4213-9125-559c21ea409d","trusted":true,"collapsed":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad97a5c8cd433ec3f0d757b73e163c47efb1d8e8","_cell_guid":"301bb743-9c81-4228-936c-32b17b57aa96"},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"_uuid":"da62e904b8e149cc3239daf8f209a9472ccb13cc","_cell_guid":"bedf7f6c-29b1-4724-9977-67f23414d320","trusted":true,"collapsed":true},"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b39e134858428fdbc7ed40a1be057f3c2e20418","_cell_guid":"16a910b2-fcc6-4204-871f-0fc524bba888"},"cell_type":"markdown","source":"### K-Nearest Neighbours(KNN)"},{"metadata":{"_uuid":"ae81fa1b7a11605a1a7366da0b08af05ab6a2662","_cell_guid":"4a840c02-24c5-4e77-98f0-6d972ce08707","trusted":true,"collapsed":true},"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d74d9689faf92c6e44f73dbde7e4992ecc53d765","_cell_guid":"8968f37a-0e0f-48b0-94f4-5018b9db6f08"},"cell_type":"markdown","source":"KNN 모델의 정확도는 **n_neighbours** 특성의 값이 변함에 따라 변한다. 기본 값은 **5** 다. n_neighbours의 다양한 값에 따라 정확성을 확인하자."},{"metadata":{"_uuid":"8920914151d70a6231389b86e609aec66a5b88c0","_cell_guid":"9633dca4-eaa2-4d59-9590-d03fe1510cde","trusted":true,"collapsed":true},"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef98b8d1030a9f68a68a5064c2980e38a448b8fe","_cell_guid":"bb36dadf-0e74-4372-adf8-751a31951570"},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"_uuid":"66889b223ce7c42880b23b6d2ec13c7a8ca8b028","_cell_guid":"fe14b93b-7732-44a3-a8fd-667abd2713fc","trusted":true,"collapsed":true},"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d1e5161b35601aeb5b44a4fcf6b88b243a6151","_cell_guid":"54cd59cb-854d-40d4-ac81-2d86a290977a"},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"_uuid":"2adaf51a647e1a2ec1d89189da53c1d8124e3d10","_cell_guid":"4a3fb4ef-213b-42f2-864d-9b758088f296","trusted":true,"collapsed":true},"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"271705b29ca8f22c789072720fde34203cf3fd1b","_cell_guid":"9d87ba93-aeaa-456b-857d-ce03229f5e29"},"cell_type":"markdown","source":"모델의 정확성은 classifier의 건장함을 결정하는데 유일한 요소는 아니다. classifier가 training data를 과하게 train되었고 test 데이터로 테스트하면 정확성을 90%이다.\nThe accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nclassifier에게 매우 좋은 정확성인 것 같다. 그러나 모든 새로운 테스트셋에도 90%일까? 답은 **아니다.** 왜냐하면 classifier가 자신을 훈련하는 데 어떤 예제를 사용할 지 결정할 수 없기 때문이다. training과 testing 데이터가 변함에 따라, 정확도 또한 변할 것이다. 증가하거나 감소할 지도 모른다. 이것은 **model variance**로 알려진다.\n\n이것을 극복하고 일반화된 모델을 얻기 위해 우리는 **Cross Validation**을 사용한다.\n\n\n# Cross Validation\n\n한 번에 여러번, 데이터는 불균형되어 있다. 예를 들어, class1 예제에는 높은 수가 있지 모르나 다른 class 예제에는 더 낮은 수가 있을 지 모른다. 따라서, 우리의 알고리즘을 각각 그리고 dataset의 모든 예제에 대해 훈련시키고 테스트해야 한다. 그러면 데이터셋에 기록된 모든 정확도들을 평균을 얻을 수 있다.\n\n1)K-Fold Cross Validation은 데이터셋을 k-subsets 으로 나누는 것을 처음으로 작동한다.\n\n2)데이터셋을 (k=5) 부분으로 나눈다고 하자. 테스트를 위한 1개의 파트를 남겨두고 4개의 부분에 대해 알고리즘을 훈련한다.\n\n3)각각의 반복에 테스트하는 부분과 다른 부분들에 대해 알고리즘을 훈련하는 부분을 바꾸면서 프로세스를 계속한다. 알고리즘의 평균 정확성을 얻기 위해 정확성과 오류들은 평균낸다.\n\n이것이 K-Fold Cross Validation이라고 불린다.\n\n4)하나의 알고리즘은 몇몇의 트레이닝 데이터에 대한 하나의 데이터셋에 언더피팅될 지도 모른다. 그리고 때떄로 다른 트레이닝 셋에 대해 데이터가 오버피팅될 지도 모른다. 따라서 cross-validation에서는 일반화된 모델을 얻을 수 있다."},{"metadata":{"_uuid":"0008f647edc90d8da7811a77394b7f97dc4084c5","_cell_guid":"6055a9d4-60c3-4f05-80ad-7b24aa7905f6","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"806ea60471bde0ee16e59826fb53dfbb1c2e33e9","_cell_guid":"34b4b0c2-8a1a-4ab4-bc3d-bc90ced71dd7","trusted":true,"collapsed":true},"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547adc3bcf5e0b6548046e1f1554c51b70dcd610","_cell_guid":"f165a7fd-e614-4577-8bc5-0903d066d22e","trusted":true,"collapsed":true},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa34471632a4a7449881f8dfedc4f4c3575f1506","_cell_guid":"37935965-7042-4828-983b-bea15524551c"},"cell_type":"markdown","source":"classification 정확성은 때떄로 불균형 떄문에 잘못될 수도 있다. confusion matrix의 도움으로 요약된 결과를 얻을 수 있다. confusion matrix는 모델이 잘 못 되고 있는 지, 또는 클래스가 틀리게 예측했는 지를 보여준다.\n\n## Confusion Matrix\n\n이것은 classifier에 의해 만들어진 정확하고 부정확한 분류의 수를 보여준다."},{"metadata":{"_uuid":"9d1a7c6efcb76a121f29a861f77e0e32e4904d4a","_cell_guid":"d6ae1291-7a55-4cc0-a039-0b9cbf2deedb","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"635c65dbd043e5e46e828c9101bf397d25717671","_cell_guid":"d8c66c23-c57c-4d3c-8742-4baa551f9ab1"},"cell_type":"markdown","source":"### Interpreting Confusion Matrix\n\n외쪽 대각선은 각각의 클래스에서 만들어진 올바른 예의 수를 보여준다. 반면에 오른쪽 대각선은 잘못 만들어진 예측의 수를 보여준다. rbf-SVN을 위한 첫번째 plot을 보자.\n\n1)올바른 예측의 수는 **491(사망) + 247(생존)**이고, 평균 CV 정확도는 이전에 얻었던 **(491+247)/891 = 82.5%** 이다.\n\n2)**오류**--> 58명의 사망자를 생존자로, 95명의 생존자를 사망자로 잘못 분류했다. 따라서 사망자를 생존자로 예측하는 데 더 많은 실수가 있었다.\n\n모든 행렬을 보면서, rbf-SVM이 사망자를 올바르게 예측할 확률이 더 높으나, NaiveBayes는 생존자를 올바르게 예측할 확률이 더 높다고 할 수 있다."},{"metadata":{"_uuid":"fbfd27e3e1feae00fe22f517d2c4ebc8247c936c","_cell_guid":"d264a15a-fcb1-49ea-a374-4b5cd9501738"},"cell_type":"markdown","source":"### Hyper-Parameters Tuning\n\n머신 러닝 모델은 블랙박스같다. 이 블랙박스를 위해서는 몇몇의 기본 파라미터 값들이 있다. 그리고 이 값들은 더 나은 모델을 얻기 위해 조정하고 바꿀 수 있다. SVM 모델의 C와 gamma처럼, 다른 classifiers들의 단순히 다른 파라미터들을hyper-parameter라고 한다. hyper-parameter는 알고리즘의 learning rate를 바꾸기거나 더 나은 모델을 얻기 위해 조절할 수 있다. 이것을 hyper-parameter 튜닝이라고 알려져있다.\n\n2개의 가장 나은 classifier인 SVM과 RandomForest에서 hyper-parameter를 조절할 것이다.\n\n#### SVM"},{"metadata":{"_uuid":"27cec77e7540f00f14f16ad654dbc3b285979450","_cell_guid":"b55abfe7-c4be-4712-b4ed-3447f18b9503","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5139677175baefbada84f8e8b080ab9ca82cad9","_cell_guid":"0105b9ed-30bd-45b6-81ab-058d4b24055a"},"cell_type":"markdown","source":"#### Random Forests"},{"metadata":{"_uuid":"73b496e28b85d890b03290352264fa5eb32e2075","_cell_guid":"651ec70e-9823-401f-8e7d-62eb29c1c3a7","trusted":true,"collapsed":true},"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37e3a5077f20b3fc2fb55141a6ca4adcd58a053","_cell_guid":"849fb470-3b99-4a8f-99ca-1117bd913347"},"cell_type":"markdown","source":"Rbf-SVM의 최고 점수는 **C=0.05, gamma=0.1일 때, 82.82%**이다.\nRandomForest에서는, 점수가 약 **n_estimators=900일 때, 81.8%**이다."},{"metadata":{"_uuid":"ddf9e42f2103a765a2b9ce0f52ac67478c36164c","_cell_guid":"108536a3-68e9-4abc-8dc7-c98b801a3386"},"cell_type":"markdown","source":"> # Ensembling\n\nEnsembling은 모델의 정확도와 성능을 높이는 좋은 방법이다. 간단히 말해서, 하나의 강력한 모델을 만들기 위해 다양하고 단순한 모델들의 조합이다.\n\n우리가 전화를 사고 싶고 다양한 파라미터들을 기반으로 그것에 대해 많은 사람들에게 묻는다고 하자. 그래서 모든 파라미터들을 분석한 후에 하나의 상품에 대한 확실한 판단을 만들 수 있다. 이것이 **Ensembling**이다. Ensembling은 모델의 안정성을 증가시킨다. Ensembling은 다음과 같은 방법으로 행해진다.\n\n1)Voting Classifier\n\n2)Bagging\n\n3)Boosting"},{"metadata":{"_uuid":"22670e16c173f051f1c2eba96a4faa83fc87053b","_cell_guid":"8c7f49d7-8986-4c75-816d-a82e1e000e34"},"cell_type":"markdown","source":"## Voting Classifier\n\n많은 다른 단순한 머신 러닝 모델로 부터 예측들을 조합하는 가장 단순한 방법이다. 이것은 모든 서브모델의 예측에 기반하여 예측 평균 결과를 가져온다. 서브 모델이나 기본 모델들은 모두 다른 형식이 있다."},{"metadata":{"_uuid":"3fde83a97a3f8ec941901886b1694941a77b740c","_cell_guid":"0fb4987a-e837-4dd6-89a5-ac8d9c47bb8c","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b1d6e720f77c19f785cb28d5bcda57eeb57e10b","_cell_guid":"79afa502-8bfd-416e-95dc-1297b9e42a39"},"cell_type":"markdown","source":"## Bagging\n\nBagging은 일반적인 ensemble 방법이다. 데이터셋에 대한 작은 부분들에 대해 비슷한 classifier를 적용하고, 모든 예측의 평균을 가지고 작동한다. 평균 때문에, variance가 감소한다. Voting Classifier와 다르게, Bagging은 비슷한 Classifier를 이용한다.\n\n#### Bagged KNN\n\nBagging은 높은 variance를 가진 모델들에 가장 좋다. 이것의 예는 결정트리나 랜덤포레스트가 될 수 있다. 우리는 **n_neghbours**의 작은 값으로 KNN을 사용할 수 있다."},{"metadata":{"_uuid":"60c5490b5804a9679629dc819caa2d8e18a7893b","_cell_guid":"aa9aa59c-417e-430a-90be-ff28f463c124","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e78fa11fc31aba7f0840fdfd837c1ada8b14c9","_cell_guid":"56a42f5c-da5e-4568-8c81-214353999328"},"cell_type":"markdown","source":"#### Bagged DecisionTree\n"},{"metadata":{"_uuid":"5b31f702c3ba63fefab40b3367b22fbf6f1a8f6d","_cell_guid":"477cf946-fb66-42c0-a824-63dced016235","trusted":true,"collapsed":true},"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d05183dac7f4c41ddeda980fe5fcf83caf3ef45","_cell_guid":"eeed9cf6-c417-44f8-8feb-6f79611e5efe"},"cell_type":"markdown","source":"## Boosting\n\nBoosting은 classifier의 연속적인 학습을 사용하는 ensemblig 기술이다. 이것은 약한 model.Boosting의 이 다음처럼 작용하는 단게적 증가이다.\n\n모델은 완벽한 데이터셋에 처음으로 훈련된다. 모델은 몇몇은 잘못된 반면에 몇몇은 올바른 예를 가질 것이다. 다음 반복에서, 학습은 잘못 예측된 예제에 집중하거나 그것에 더 많은 가중치를 주는 것에 집중할 것이다. 따라서 틀린 예제를 올바르게 예측하려고 노력할 것이다. 정확성의 한계까지 도달할 때까지, 이런 반복적인 처리를 계속하고, 새로운 classifier는 모델에 추가된다."},{"metadata":{"_uuid":"1b36f078809acf52842a01c7de5afbbe8000badc","_cell_guid":"5fd768e5-78ce-45b4-a015-2994cd003de9"},"cell_type":"markdown","source":"#### AdaBoost(Adaptive Boosting)\n\n이런 경우에, 약한 learner나 estimator를 가지고 있는 것은 결정 트리다. 그러나 우리가 선택한 알고리즘에 기본 base_estimator를 바꿀 수 있다."},{"metadata":{"_uuid":"32f72bdb2b9b054f7b7ef839dab62e430a2d050d","_cell_guid":"a0a08d75-57bd-4c7d-a3b6-68cba2ba915b","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b89e596f6b3fbc3441abf75e676060c12289b26e","_cell_guid":"898b737c-37a4-4c5b-a745-e39351a0b790"},"cell_type":"markdown","source":"#### Stochastic Gradient Boosting\n\n여기도 또한 약한 learner가 결정트리이다."},{"metadata":{"_uuid":"a7aaea0c740932c4d7248fab80012553feefb0e2","_cell_guid":"b41cd90a-04d4-4e8b-afe3-c50a66dda140","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1357e5586025b3ed8facde947ca86802f4815b5","_cell_guid":"21565e7d-82bc-462f-b50c-4486a2a1dc69"},"cell_type":"markdown","source":"#### XGBoost"},{"metadata":{"_uuid":"a0a6b1f8b56a578a1a5e90da72e7e7a31f5ef1f3","_cell_guid":"a0f54823-8c1a-4287-845a-cf2749bbd243","trusted":true,"collapsed":true},"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"750f3084480a868530e2b788edc91c54dfc5ac8d","_cell_guid":"6c163453-2257-487a-b9f1-df4cc159f2ae"},"cell_type":"markdown","source":"우리는 가장 AdaBoost에 가장 높은 정확도를 얻었다. 우리는 Hyper-parameter 튜닝을 통해 정확도를 증가시킬 것이다.\n\n#### Hyper-Parameter Tuning for AdaBoost"},{"metadata":{"_uuid":"4fcae3c275c3a8a5618727b2d526c996aacb7c5e","_cell_guid":"5cf49532-71b9-4d67-911a-581ddfc47730","trusted":true,"collapsed":true},"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cbeb738e5cb865d90753e77f12dae18d3d06a65","_cell_guid":"3736f533-0a16-4881-99d2-3a7275935d47"},"cell_type":"markdown","source":"AdaBoost를 통해 얻을 수 있는 최고 정확도는 **n_estimators=200, learning_rate=0.05일 때, 83.16%**이다."},{"metadata":{"_uuid":"107b5a93285d783a08beb89795c21a665d7dbb7c","_cell_guid":"e1cb4442-2ddd-4854-9e62-7ff4543e8c9c"},"cell_type":"markdown","source":"### Confusion Matrix for the Best Model"},{"metadata":{"_uuid":"3d9debcdbd9d7b9c4e88c214385e8a6b5ba7f8fa","_cell_guid":"337cee47-1ebb-4ac3-866e-f087bed5c4a3","trusted":true,"collapsed":true},"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc679469ae2c8d73b1d42d56a36c390fc12017e","_cell_guid":"1daa7ac2-03e6-4b87-b627-f73a03eb6c35"},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"_uuid":"1c11bc8983157d3041b2144fffe0bb27992aa9a1","_cell_guid":"1aa1f4b8-f625-4fad-9f70-8b61c39a7f1a","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03663c4786700a6d4083b4aa1a0f6d913725fefb","_cell_guid":"57661993-cb09-4eac-b4f4-8666f3e1a1b7"},"cell_type":"markdown","source":"우리는 다양한 랜덤포레스트나 adaBoost 등 같은 다양한 classifiers들을 위한 중요한 feature를 알 수 있다.\n\n#### Observations:\n\n1)공통의 중요한 feature들은 Initial, Fare_cat, Pclass, Family_Size이다.\n\n2)성별 feature는 중요성을 주는 것 같지 않다. 우리가 이전에 알았던 것처럼 충격적이다. Pclass와 조합된 성별이 매우 좋은 차이를 만들어내는 요소를 주고 있었다. 성별은 오직 랜덤포레스트에서만 중요한 것처럼 보인다.\n\n그러나, feature Initial을 보면, 많은 classifier에서 최고이다. 우리는 성별과 Initial사이에 이미 양의 상관관계를 봤었다. 그래서 둘 다 gender를 의미하는 것이다.\n\n3)비슷하게, Pclass와 Fare_cat은 승객의 지위를 나타내고, Family_Size는 개인,가족,형제를 나타낸다."},{"metadata":{"_uuid":"9b10931b25a97196ebc0496f6d72246a75ad1349","_cell_guid":"bef630bd-03ef-476e-a805-ac81b9ca3f54"},"cell_type":"markdown","source":"I hope all of you did gain some insights to Machine Learning. Some other great notebooks for Machine Learning are:\n1) For R:[Divide and Conquer by Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82297/notebook)\n\n2)For Python:[Pytanic by Heads and Tails](https://www.kaggle.com/headsortails/pytanic)\n\n3)For Python:[Introduction to Ensembling/Stacking by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)\n\n### Thanks a lot for having a look at this notebook. If you found this notebook useful, **Do Upvote**.\n"},{"metadata":{"_uuid":"17eaa04af162b6cd0745f932537079995c771278","collapsed":true,"_cell_guid":"c0cd2eff-759d-46dd-9857-716834ca62d5","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}